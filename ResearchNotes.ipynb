{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All code for this project can be found at https://github.com/marcelaf7/jungle-chess-reinforcement-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes To Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my proposal, I originally planned to create an agent to play the game Jungle Chess. I planned to write the agent using a reinforcement learning algorithm. I wanted train the agent three different ways, with an opponent that makes completely random moves, and opponent that makes the best moves, and a human opponent. I also wanted the final, trained agent to be able to play the game with 3 different diffuculty levels, that of a beginner player, an intermediate player, and a professional player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This proposal was extremely ambitious given the time I had and the algorithm used. I changed project to instead focus on getting the agent to work with maximum difficulty by only training it against an opponent who makes completely random moves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Went Wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest problems I faced with this project is that the state of game cannot be simplified. Because of this, the Q table has to store every possible location of every piece, and every combination of those locations. Along with every possible move that can be made from every state. I did not take the time to calculate how many possible states and moves that would be, but it is well over 5 million. I realized early on that this much data would not be able to fit in to memory.\n",
    "\n",
    "My solution storing so much data without fitting it into memory is to use a mySQL database. With a database, it can search through the information quickly without having to store all of it into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with the memory issues fixed, training the agent could take an inanely long time. So long that the agent could train non-stop for months and only train on a fraction of the possible states. There are three issues that come into play here. The complexity of the game allows for two players to infinitely make random moves without ever wining, the random number generator could create loops, and in order for the agent to see every state, it would need to make some very specific moves.\n",
    "\n",
    "Jungle Chess if fairly complicated. It is possible for the agent and its opponent to makes millions of moves without ever advancing the game. The ways to win are either to defeat all of the opponent's pieces, or to get one piece in to the opponent's den. It is entirely possible and not unlikely for the agent and its opponent to make moves that never attack another piece or move a piece closer to the opponent's den.\n",
    "\n",
    "Along with the complexity, making random moves could cause the agent and its opponent to loop infinitely. Making random moves means that there is no guarantee a different move will be made every time. Depending a little on how Java's Math.random() function works, it is possible that the agent will move one piece, and immediately move it back. This could potentially go on infinitely and there is nothing in place to prevent this from happening.\n",
    "\n",
    "Finally, the agent would need to play every possible state and every possible move in order to be fully trained. A lot of states would require some very specific combinations of moves to happen. With so many possible states and moves, it is completely possible for the agent to never see too many states. Additionally, in order to really learn, the agent would need to see these states multiple times. One possible solution to this issue is to have the agent make a random move when it finds a state that is not in the Q Table, but the agent would not meet the goal of being the best possible Jungle Chess player it can be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Went Well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing code did go pretty well. The way we structured the code when creating the game made it fairly easy to get the information that reinforcement learning needs. Our structure made it easy to get the locations of all the pieces and we already had methods for getting a list of valid moves and making a move. The database was also easy to set up. The JDBC library made it easy for to make methods that work very similarly to the dictionary implementation that was used in the example code. With these in place, the algorithm did not need to be changed much and did not take more work than necessary to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What I Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I started this project, I was aware that Q learning is generally a bad idea if the state is complicated and there is no way to simplify it. For example, if it were possible to simplify the state to just be which pieces are near a specific piece, there would be much less states and they might actually be able to fit into memory. I don't believe that this specific situation would work well for Jungle Chess, and I could not think of a different way to simplify it. Regardless, I still wanted to try it and see if I could make it work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the project, I learned a lot about reinforcement learning. I learned that a Q table implementation does not work well when there are too many possible states and moves. The only way to implement it well with complicated problems is to find a creative way to simplify the state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
